{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a148c2a",
   "metadata": {},
   "source": [
    "# Spotify Dataset Exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d21c0",
   "metadata": {},
   "source": [
    "**Comment:** \n",
    "\n",
    "1. Objective: \n",
    "    1. To only focus ON ARTISTS that WERE NOT LISTENED TO or SUCCESSFUL in 2014. \n",
    "    2. To limit to artists who released songs between 2015 to 2017.\n",
    "    3. The focus can be shifted to SONGS that were released between 2015 and 2017, BUT WERE NOT SUCCESSFUL in 2014. \n",
    "    4. We should add more playlists to the \"success\" criteria - https://www.complex.com/music/best-spotify-playlists/new-music-friday\n",
    "    5. Perhaps we can make the top 20 playlists the success criteria\n",
    "    6. We should take genre specifics into considerations as well (RapCaviar, Massive Pop Remixes, etc.). Major genres should be explored\n",
    "    \n",
    "2. Limitations:\n",
    "    1. We can't observe the direct effect of certain playlists on music on a timeline, as the day is always 10.\n",
    "    2. The weeks are always at fixed intervals. \n",
    "    3. The success criteria has been determined by WMG - we might need to broaden our definition of 'success'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be6c649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment:\n",
    "\n",
    "# !pip install xgboost\n",
    "# !pip install tensorflow\n",
    "# !pip install \"tensorflow-text==2.8.*\"\n",
    "# !pip install bokeh\n",
    "# !pip install simpleneighbors[annoy]\n",
    "# !pip install tqdm\n",
    "# !pip install lyricsgenius\n",
    "# !pip install spotipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1088405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b28e893f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Relevant Packages:\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import Isomap, LocallyLinearEmbedding, TSNE\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, accuracy_score, precision_recall_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import pickle \n",
    "import time\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33339956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages for Lyrics Embeddings:\n",
    "\n",
    "import bokeh\n",
    "import bokeh.models\n",
    "import bokeh.plotting\n",
    "import os\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import sklearn.metrics.pairwise\n",
    "from simpleneighbors import SimpleNeighbors\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e38442",
   "metadata": {},
   "source": [
    "## Some Sources:\n",
    "\n",
    "https://output.com/blog/playlists-good-or-bad-for-musicians#:~:text=Playlists%20match%20the%20right%20song%20to%20the%20right%20listener&text=Often%20without%20promotion%2C%20these%20customized,likely%20want%20to%20hear%20it.\n",
    "\n",
    "https://github.com/maxgmarin/AC209a_FinalProject_EEM\n",
    "\n",
    "https://github.com/maxgmarin/AC209a_FinalProject_EEM/blob/master/notebook_Markdown/AC209a_Final_ER_Spotify_EDA.md\n",
    "\n",
    "https://www.theinformationlab.co.uk/2019/08/08/getting-audio-features-from-the-spotify-api/\n",
    "\n",
    "https://towardsdatascience.com/using-sentence-embeddings-to-explore-the-similarities-and-differences-in-song-lyrics-1820ac713f00\n",
    "\n",
    "https://medium.com/swlh/how-to-leverage-spotify-api-genius-lyrics-for-data-science-tasks-in-python-c36cdfb55cf3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f65c90",
   "metadata": {},
   "source": [
    "### Ideas to Walk though:\n",
    "\n",
    "1. Some artists may have JUST dropped their FIRST song. Or they may have been in the industry for far less years. \n",
    "2. We may need more characteristics regarding particular playlists. \n",
    "3. Correlation Matrix on SoundTrack features can give us an idea of whether to do a PCA on all of them, or each of them\n",
    "4. Playlist features can be done based on average of songs (IN THE DATASET RIGHT NOW) on particular playlists. \n",
    "5. We can investigate genre on PCA of the audio features later - https://maxgmarin.github.io/AC209a_FinalProject_EEM/\n",
    "6. Maybe get twitter posts or reddit posts regarding songs for 2017 and before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23a9a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9112da57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>day</th>\n",
       "      <th>log_time</th>\n",
       "      <th>mobile</th>\n",
       "      <th>track_id</th>\n",
       "      <th>isrc</th>\n",
       "      <th>upc</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>...</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>weekday_name</th>\n",
       "      <th>playlist_id</th>\n",
       "      <th>playlist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>('small_artists_2016.csv', 9)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T12:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>('small_artists_2016.csv', 19)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T12:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>('small_artists_2016.csv', 29)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T14:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>('small_artists_2016.csv', 39)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T10:45:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>('small_artists_2016.csv', 49)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T10:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>59</td>\n",
       "      <td>('small_artists_2016.csv', 59)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T02:30:00</td>\n",
       "      <td>False</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>('small_artists_2016.csv', 69)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T09:45:00</td>\n",
       "      <td>False</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>79</td>\n",
       "      <td>('small_artists_2016.csv', 79)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T14:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>89</td>\n",
       "      <td>('small_artists_2016.csv', 89)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T19:15:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>99</td>\n",
       "      <td>('small_artists_2016.csv', 99)</td>\n",
       "      <td>10</td>\n",
       "      <td>20160510T15:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>8f1924eab3804f308427c31d925c1b3f</td>\n",
       "      <td>USAT21600547</td>\n",
       "      <td>7.567991e+10</td>\n",
       "      <td>Sturgill Simpson</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-05-10</td>\n",
       "      <td>1</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                  Unnamed: 0.1.1  day  \\\n",
       "0           0             9   ('small_artists_2016.csv', 9)   10   \n",
       "1           1            19  ('small_artists_2016.csv', 19)   10   \n",
       "2           2            29  ('small_artists_2016.csv', 29)   10   \n",
       "3           3            39  ('small_artists_2016.csv', 39)   10   \n",
       "4           4            49  ('small_artists_2016.csv', 49)   10   \n",
       "5           5            59  ('small_artists_2016.csv', 59)   10   \n",
       "6           6            69  ('small_artists_2016.csv', 69)   10   \n",
       "7           7            79  ('small_artists_2016.csv', 79)   10   \n",
       "8           8            89  ('small_artists_2016.csv', 89)   10   \n",
       "9           9            99  ('small_artists_2016.csv', 99)   10   \n",
       "\n",
       "            log_time  mobile                          track_id          isrc  \\\n",
       "0  20160510T12:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "1  20160510T12:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "2  20160510T14:00:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "3  20160510T10:45:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "4  20160510T10:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "5  20160510T02:30:00   False  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "6  20160510T09:45:00   False  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "7  20160510T14:00:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "8  20160510T19:15:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "9  20160510T15:00:00    True  8f1924eab3804f308427c31d925c1b3f  USAT21600547   \n",
       "\n",
       "            upc       artist_name  ... hour minute week month  year  \\\n",
       "0  7.567991e+10  Sturgill Simpson  ...   12     15   19     5  2016   \n",
       "1  7.567991e+10  Sturgill Simpson  ...   12     15   19     5  2016   \n",
       "2  7.567991e+10  Sturgill Simpson  ...   14      0   19     5  2016   \n",
       "3  7.567991e+10  Sturgill Simpson  ...   10     45   19     5  2016   \n",
       "4  7.567991e+10  Sturgill Simpson  ...   10     15   19     5  2016   \n",
       "5  7.567991e+10  Sturgill Simpson  ...    2     30   19     5  2016   \n",
       "6  7.567991e+10  Sturgill Simpson  ...    9     45   19     5  2016   \n",
       "7  7.567991e+10  Sturgill Simpson  ...   14      0   19     5  2016   \n",
       "8  7.567991e+10  Sturgill Simpson  ...   19     15   19     5  2016   \n",
       "9  7.567991e+10  Sturgill Simpson  ...   15      0   19     5  2016   \n",
       "\n",
       "         date weekday  weekday_name playlist_id playlist_name  \n",
       "0  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "1  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "2  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "3  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "4  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "5  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "6  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "7  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "8  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "9  2016-05-10       1       Tuesday         NaN           NaN  \n",
       "\n",
       "[10 rows x 45 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('newartists2015onwards.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19ba67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove further useless columns:\n",
    "\n",
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1', 'Unnamed: 0.1.1'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7f1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ones with high % nulls\n",
    "\n",
    "df.drop(['offline_timestamp', 'stream_cached', 'source', 'referral_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128128fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399057fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(df.head().to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274c61b",
   "metadata": {},
   "source": [
    "**Comment:** \n",
    "\n",
    "1. We will need all type-based features\n",
    "2. For each artist, we will get the features of the 10 most played songs that they have. \n",
    "3. There will be a table for each feature, and the rows will be organised by artist\n",
    "4. A PCA will be done on all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of songs\n",
    "df.track_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most played song\n",
    "df.track_name.value_counts().loc[lambda x: x == x.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cd934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of songs played less than 10 times\n",
    "len(df.track_name.value_counts().loc[lambda x: x < 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1af054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of artists\n",
    "df.track_artists.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299616e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artists with at least 5 songs\n",
    "no_of_tracks = df[['track_artists', 'track_name']].drop_duplicates().groupby(by='track_artists').count().sort_values(by='track_name', ascending=False)\n",
    "no_of_tracks.reset_index(inplace=True)\n",
    "artists_with_5plus_tracks = no_of_tracks[no_of_tracks['track_name'] >= 5]\n",
    "artists_with_10plus_tracks = no_of_tracks[no_of_tracks['track_name'] >= 10]\n",
    "print('Number of artists with 5 or more songs: ', len(artists_with_5plus_tracks))\n",
    "print('Number of artists with 10 or more songs: ', len(artists_with_10plus_tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playlists available\n",
    "df.playlist_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933dfe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 playlists by number of songs and number of artists\n",
    "constraint = df['playlist_name'].notna()\n",
    "df[constraint][['playlist_name', \n",
    "                'track_name']].drop_duplicates().groupby(by='playlist_name').count().sort_values(by='track_name', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artists organised by how many playlists they are on\n",
    "df[constraint][['artist_name', \n",
    "                'playlist_name']].drop_duplicates().groupby(by='artist_name').count().sort_values(by='playlist_name', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective playlists:\n",
    "# success_playlists = ['Hot Hits UK', 'Massive Dance Hits', 'The Indie List', 'New Music Friday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0119f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of artists by when their songs were played:\n",
    "\n",
    "df.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c12908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee56ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2a4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dffa09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "754d6c19",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "1. We will have to first gather which artists have made it onto the successful playlists\n",
    "2. After that, we drop columns with such playlists, in order to avoid feature leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c6714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all artists in ascending order\n",
    "all_artists_ordered = df['artist_name'].drop_duplicates().sort_values()\n",
    "len(all_artists_ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98521361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all artists that have been successful\n",
    "temp_df = df[df.playlist_name.isin(success_playlists)][['artist_name', 'playlist_name']]\n",
    "\n",
    "# artists by degree of success\n",
    "artists_dos = temp_df.copy().drop_duplicates().groupby('artist_name').count().reset_index()\n",
    "artists_dos.columns = ['artist_name', 'degree_of_success']\n",
    "artists_dos.sort_values(by='artist_name', inplace=True)\n",
    "\n",
    "# artists that reached success\n",
    "artists_rs = temp_df.drop_duplicates(subset='artist_name').copy()\n",
    "artists_rs.columns = ['artist_name', 'success']\n",
    "artists_rs.loc[:, 'success'] = 1\n",
    "artists_rs.sort_values(by='artist_name', inplace=True)\n",
    "\n",
    "# display the dataframes\n",
    "display(artists_dos.head(5))\n",
    "display(artists_rs.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb529de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All artists, whether they have reached success or not\n",
    "artists_status = [1 if x in artists_rs.artist_name.to_list() else 0 for x in all_artists_ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be57c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist linked to dependent variable - reached success or not\n",
    "\n",
    "# artist linked to dependent variable - degree of success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1330f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.partner_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many songs are remixes\n",
    "list_of_tracks = df.track_name.drop_duplicates().tolist()\n",
    "        \n",
    "len([song for song in list_of_tracks if ('Remix' in song) or ('remix' in song)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73715272",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([song for song in list_of_tracks if ('Remix' in song) and ('-' not in song)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.track_name.str.contains('Remix')]['track_name'].drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.track_name.str.contains('Remix')]['track_name'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24538de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c71a5e",
   "metadata": {},
   "source": [
    "# Checking Which Songs were Listened To By Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d31fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3eace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Song/Year Pivot Table\n",
    "\n",
    "df_song_year = pd.pivot_table(df, values='track_id', index='track_name', columns = 'year', aggfunc=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43323361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling and showing\n",
    "\n",
    "df_song_year.fillna(0, inplace=True)\n",
    "\n",
    "df_song_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600b8b1",
   "metadata": {},
   "source": [
    "# Defining Success Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(temp, values='track_id', index='date', columns='track_name', aggfunc=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e21ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = df[(df.track_name == '7 Years') & (df.year == 2017)]\n",
    "sns.countplot(x='day', data=temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afa0bb",
   "metadata": {},
   "source": [
    "### Checking Which Playlist IDs are Counted as SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist_name_id_pairs = df[['playlist_id', 'playlist_name']].copy().drop_duplicates()\n",
    "top_100_playlists = df.playlist_id.value_counts().head(100).to_frame().reset_index()\n",
    "top_100_playlists.columns = ['playlist_id', 'stream_count']\n",
    "top_100_playlists = top_100_playlists.merge(playlist_name_id_pairs, on='playlist_id', how='left', copy=True)\n",
    "top_100_playlists.playlist_name = top_100_playlists.playlist_name.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762f0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_playlists[top_100_playlists['playlist_name'].isin(['Hot Hits UK', 'Massive Dance Hits', 'The Indie List', 'New Music Friday UK'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3b3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order is Hot Hits UK, Massive Dance Hits, New Music Friday UK, The Indie List\n",
    "\n",
    "success_playlist_ids = ['6FfOZSAN3N6u7v81uS7mxZ', '37i9dQZF1DX5uokaTN4FTR', '37i9dQZF1DX4W3aJJYCDfV', '37i9dQZF1DWVTKDs2aOkxu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd920c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for Success Criteria\n",
    "\n",
    "success_criteria_df = top_100_playlists[top_100_playlists['playlist_id'].isin(success_playlist_ids)]\n",
    "success_criteria_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4caa9c",
   "metadata": {},
   "source": [
    "### Checking Which Playlist IDs should be Ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1970d97",
   "metadata": {},
   "source": [
    "**Rule:** We will remove playlists with a stream count of above 10k, and the playlists that are considered as a hallmark of success as well. This is to mitigate feature leakage, and to further inhibit using variables that are correlated with the outcome variable as a predictor variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists_to_be_ignored = top_100_playlists[top_100_playlists.stream_count > 10000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fe66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending success playlist on top, and removing duplicates\n",
    "\n",
    "playlists_to_be_ignored = pd.concat([playlists_to_be_ignored.copy(), success_criteria_df.copy()]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d539d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To CSV\n",
    "\n",
    "# success_criteria_df.to_csv('playlists_success_criteria.csv')\n",
    "# playlists_to_be_ignored.to_csv('playlists_to_ignore_PCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912aeffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d48fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5333f82e",
   "metadata": {},
   "source": [
    "# Checking Which Artists were Successful Across the Years (And to Degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the necessary values of main df\n",
    "\n",
    "logged_success = df[df.playlist_name.isin(success_playlists)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pivot ready with filled values\n",
    "\n",
    "df_successes_year = pd.pivot_table(logged_success, values='playlist_id', index='artist_name', columns = 'year', aggfunc=pd.Series.nunique)\n",
    "df_successes_year.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93600ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See dataframe\n",
    "\n",
    "df_successes_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_success.playlist_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bf782",
   "metadata": {},
   "outputs": [],
   "source": [
    "logged_success.playlist_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8621b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88c6c5a5",
   "metadata": {},
   "source": [
    "# SpotiPy and Genius (With geniuslyrics Python Wrapper) API Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4eef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Spotify\n",
    "\n",
    "import requests\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cc6408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spotipy Credentials\n",
    "\n",
    "CLIENT_ID = '1c30003afc8142c3bf686eca75a3af8c'\n",
    "CLIENT_SECRET = '338740b73889449199d83a1cf1424d06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d8444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spotify non-user initialisation\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "sp = spotipy.Spotify(client_credentials_manager = client_credentials_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Songs\n",
    "\n",
    "df.track_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1848ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making list of unique track URIs to reference\n",
    "\n",
    "track_uri_list = df.track_uri.copy().drop_duplicates().tolist()\n",
    "len(track_uri_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b2163",
   "metadata": {},
   "source": [
    "### Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae67829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_features_extract(track_uri_list):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Creates a dataframe of track URIs, matched with their respective audio features. \n",
    "    This will need to be merged with song names and artist for a more complete dataframe\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reference = track_uri_list\n",
    "    danceability = []\n",
    "    energy = []\n",
    "    key = []\n",
    "    loudness = []\n",
    "    mode = []\n",
    "    speechiness = []\n",
    "    acousticness = []\n",
    "    instrumentalness = []\n",
    "    liveness = []\n",
    "    valence = []\n",
    "    tempo = []\n",
    "    duration_ms = []\n",
    "    time_signature = []\n",
    "    \n",
    "    for uri in reference:\n",
    "        temp_uri_info = sp.audio_features(uri)[0]\n",
    "        danceability.append(temp_uri_info['danceability'])\n",
    "        energy.append(temp_uri_info['energy'])\n",
    "        key.append(temp_uri_info['key'])\n",
    "        loudness.append(temp_uri_info['loudness'])\n",
    "        mode.append(temp_uri_info['mode'])\n",
    "        speechiness.append(temp_uri_info['speechiness'])\n",
    "        acousticness.append(temp_uri_info['acousticness'])\n",
    "        instrumentalness.append(temp_uri_info['instrumentalness'])\n",
    "        liveness.append(temp_uri_info['liveness'])\n",
    "        valence.append(temp_uri_info['valence'])\n",
    "        tempo.append(temp_uri_info['tempo'])\n",
    "        duration_ms.append(temp_uri_info['duration_ms'])\n",
    "        time_signature.append(temp_uri_info['time_signature'])\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    df = pd.DataFrame({'track_uri': reference, 'danceability': danceability, \n",
    "                       'energy': energy, 'key': key, 'loudness': loudness, \n",
    "                       'mode': mode, 'speechiness': speechiness, 'acousticness': acousticness, \n",
    "                       'instrumentalness': instrumentalness, 'liveness': liveness, \n",
    "                       'valence': valence, 'tempo': tempo, 'duration_ms': duration_ms, \n",
    "                       'time_signature': time_signature})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe of audio features\n",
    "\n",
    "audio_features = audio_features_extract(track_uri_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for bugs and nulls\n",
    "\n",
    "display(audio_features.head())\n",
    "display(audio_features.tail())\n",
    "print('Number of nulls: ', df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7fe0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to dataframe audio_features.csv\n",
    "\n",
    "# audio_features.to_csv('audio_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a03c24",
   "metadata": {},
   "source": [
    "### Lyrics from Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3146be20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential files \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import lyricsgenius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92959c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get song names and artist names set up as pairs and zips to draw on\n",
    "\n",
    "track_artist_pairs = df[['track_name', 'artist_name']].copy().drop_duplicates()\n",
    "\n",
    "#Getting all remixes: \n",
    "pairs_remix = track_artist_pairs[(track_artist_pairs['track_name'].str.contains('Remix')) | \n",
    "                                 (track_artist_pairs['track_name'].str.contains('remix'))]\n",
    "track_names_list_remix = pairs_remix.track_name.tolist()\n",
    "artist_names_list_remix = pairs_remix.artist_name.tolist()\n",
    "zipped_tracks_artists_remix = zip(track_names_list_remix, artist_names_list_remix)\n",
    "\n",
    "#Getting all nonremixes: \n",
    "pairs_nonremix = track_artist_pairs[~((track_artist_pairs['track_name'].str.contains('Remix')) | \n",
    "                                (track_artist_pairs['track_name'].str.contains('remix')))]\n",
    "track_names_list_nonremix = pairs_nonremix.track_name.tolist()\n",
    "artist_names_list_nonremix = pairs_nonremix.artist_name.tolist()\n",
    "zipped_tracks_artists_nonremix = zip(track_names_list_nonremix, artist_names_list_nonremix)\n",
    "\n",
    "# Get results\n",
    "print('Number of pairs: ', len(track_artist_pairs))\n",
    "print('Number of pairs that are remixes: ', len(pairs_remix))\n",
    "print('Number of pairs that are original: ', len(pairs_nonremix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aec765",
   "metadata": {},
   "source": [
    "**comment**: The nonremixes, or 'originals', should be available on the Genius API database. The remixes might not, and the API might draw on irrelevant songs. The original songs can be close to easily automated. The remixes might not. We will look into the remixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b86bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_remix.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac67eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_nonremix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97ee7cf",
   "metadata": {},
   "source": [
    "**comment:** There were some errors in regards to using the Genius API on remixes and even nonremixes. After experimentation, it seems 'feat', parentheses and hyphens interrupt the API. We will remove these from both remixes and non-remixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d9f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialising Genius API with token\n",
    "\n",
    "genius = lyricsgenius.Genius(\"cudPmM1MC6Mt5TX8vuZj6ZFgV1Zv75PL_mOy6Re6JkDEgM23lXWK1KWvTX9lqhf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fec085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "# song = genius.search_song(\"Save Me \", 'The Parakit')\n",
    "# print(song.lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function for extracting songs\n",
    "\n",
    "def extracting_lyrics(zipped_pairs):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Extracts lyrics with errors in mind, should the song not be in Genius's database. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    artist_names = []\n",
    "    track_names = []\n",
    "    lyrics = []\n",
    "    \n",
    "    for track, artist in zipped_pairs:\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "        # Setting up important variables\n",
    "        artist_temp = artist \n",
    "        if \"(\" in track:\n",
    "            track_temp = track.split('(')[0]\n",
    "        elif \"-\" in track:\n",
    "            track_temp = track.split('-')[0]\n",
    "        else:\n",
    "            track_temp = track\n",
    "        \n",
    "        # Getting the lyrics\n",
    "        try: \n",
    "            song = genius.search_song(track_temp, artist_temp)\n",
    "            lyrics.append(song.lyrics)\n",
    "            track_names.append(track)\n",
    "            artist_names.append(artist)\n",
    "        except Exception:\n",
    "            lyrics.append('Fail')\n",
    "            track_names.append(track)\n",
    "            artist_names.append(artist)\n",
    "            continue\n",
    "        \n",
    "    print('Number of lyrics we have', len(lyrics))\n",
    "    print('Number of artists we have', len(artist_names))\n",
    "    print('Number of tracks we have', len(track_names))\n",
    "    \n",
    "    # We return the lists, which we will manually turn into a dataframe\n",
    "    # This is because the function can be prone to errors, and the lists might have different lengths\n",
    "    return artist_names, track_names, lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128b6e4",
   "metadata": {},
   "source": [
    "### Original / Non-remix Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290a612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Running function for original songs\n",
    "\n",
    "# artist_names_func1, track_names_func1, lyrics_func1 = extracting_lyrics(zipped_tracks_artists_nonremix) # Takes about one hour to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada6121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(artist_names_func1[-1])\n",
    "# print(track_names_func1[-1])\n",
    "# print(lyrics_func1[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05373860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of original songs from first run that has SUCCESSFULLY retrieved lyrics:\n",
    "\n",
    "print('Number of original songs whose lyrics were successfully extracted: ', sum([1 for x in lyrics_func1 if x != 'Fail']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5780994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of original songs from first run that have FAILED retrieved lyrics:\n",
    "\n",
    "print('Number of original songs whose lyrics were failed to be extracted: ', sum([1 for x in lyrics_func1 if x == 'Fail']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d17e2f",
   "metadata": {},
   "source": [
    "**Comment:** Used the last row to see if the details of the last song aligns. It does! We can safely turn it into a dataframe. We will save it as a csv file, in order to retain our findings, as it was a time-intensive process. We will save it as 'song_lyrics_func1.csv'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lists as a dataframe, and then saving that to a csv\n",
    "\n",
    "# song_lyrics_func1 = pd.DataFrame({'track_name': track_names_func1, 'artist_name': artist_names_func1, 'lyrics': lyrics_func1})\n",
    "# song_lyrics_func1.to_csv('song_lyrics_func1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1731622a",
   "metadata": {},
   "source": [
    "### Remix Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1448b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running function for original songs (Best not to run - Load dataset in next cell)\n",
    "\n",
    "# Run function\n",
    "artist_names_func2, track_names_func2, lyrics_func2 = extracting_lyrics(zipped_tracks_artists_remix) # Takes about 30 minutes to run\n",
    "\n",
    "# Save to Dataframe\n",
    "song_lyrics_func2 = pd.DataFrame({'track_name': track_names_func2, 'artist_name': artist_names_func2, 'lyrics': lyrics_func2})\n",
    "\n",
    "# Save to csv\n",
    "# song_lyrics_func2.to_csv('song_lyrics_func2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8426f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of remix songs from first run that has SUCCESSFULLY retrieved lyrics:\n",
    "\n",
    "print('Number of remix songs whose lyrics were successfully extracted: ', sum([1 for x in lyrics_func2 if x != 'Fail']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c92a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of remix songs from first run that have FAILED retrieved lyrics:\n",
    "\n",
    "print('Number of remix songs whose lyrics were failed to be extracted: ', sum([1 for x in lyrics_func2 if x == 'Fail']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3c2b4",
   "metadata": {},
   "source": [
    "### Analysing the Failed Lyric Extractions (func1 & func2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the necessary data:\n",
    "song_lyrics_func1 = pd.read_csv('song_lyrics_func1.csv')\n",
    "song_lyrics_func2 = pd.read_csv('song_lyrics_func2.csv')\n",
    "song_lyrics_func1.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "song_lyrics_func2.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9fd95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check Func1 Table Fails\n",
    "\n",
    "song_lyrics_func1[song_lyrics_func1['lyrics'] == 'Fail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1571d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_lyrics_func1[song_lyrics_func1['track_name'].str.contains('Beethoven') |\n",
    "                   song_lyrics_func1['track_name'].str.contains('Instrumental') |\n",
    "                   song_lyrics_func1['track_name'].str.contains('Violin') |\n",
    "                   song_lyrics_func1['track_name'].str.contains('Piano')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b590f8e",
   "metadata": {},
   "source": [
    "**Comment:** These lyrics will need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "song_lyrics_func1[song_lyrics_func1['lyrics'] == 'Fail'].artist_name.value_counts().tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# song_lyrics_func1[song_lyrics_func1['lyrics'] == 'Fail'].to_csv('help_with_lyrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Func2 Table Fails\n",
    "\n",
    "song_lyrics_func2[song_lyrics_func2['lyrics'] == 'Fail']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a4ac86",
   "metadata": {},
   "source": [
    "**Comment:** For the remixes, there are only two artists that there are issues with. Perhaps another website might have the lyrics for these two artists, or another API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee4a166",
   "metadata": {},
   "source": [
    "# Processing the Audio Features of All Songs and Playlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318974ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and processing data\n",
    "\n",
    "audio_features = pd.read_csv('audio_features.csv')\n",
    "audio_features.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Perhaps we don't need some columns (probably unveiled in EDA section)\n",
    "audio_features.drop(['duration_ms', 'key'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5607f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a0ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f901c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting joined table ready\n",
    "\n",
    "audio_joined = df[['track_uri', 'playlist_id', 'artist_name', 'playlist_name']].copy().merge(audio_features, how='left', on='track_uri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pivot tables ready\n",
    "\n",
    "danceability_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='danceability', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "energy_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='energy', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "loudness_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='loudness', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "mode_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='mode', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "speechiness_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='speechiness', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "acousticness_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='acousticness', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "instrumentalness_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='instrumentalness', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "liveness_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='liveness', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "valence_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='valence', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "tempo_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='tempo', aggfunc=np.mean).fillna(0)\n",
    "\n",
    "time_signature_pivot = pd.pivot_table(audio_joined, index='artist_name', \n",
    "                                    columns='playlist_id', values='time_signature', aggfunc=np.median).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full join on all pivot tables\n",
    "\n",
    "final_audio_pivot = pd.concat([danceability_pivot, energy_pivot, loudness_pivot, mode_pivot, \n",
    "                               speechiness_pivot, acousticness_pivot, instrumentalness_pivot, \n",
    "                               liveness_pivot, valence_pivot, tempo_pivot, time_signature_pivot], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that lead to feature leakage\n",
    "\n",
    "final_audio_pivot.drop(playlists_to_be_ignored.playlist_id.tolist(), axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10446dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing PCA:\n",
    "\n",
    "pca = Pipeline([('scaler', StandardScaler()), ('pca', PCA(n_components=50))])\n",
    "components = pca.fit(final_audio_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3081ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning variables\n",
    "\n",
    "no_of_components = list(range(1,51))\n",
    "exp_variance_ratio = pca[1].explained_variance_ratio_.tolist()\n",
    "cum_exp_variance = np.cumsum(exp_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43af7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting explained variance and cumulative variance over components\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(x=no_of_components, y=exp_variance_ratio, color='green')\n",
    "sns.lineplot(x=no_of_components, y=cum_exp_variance, color = 'orange')\n",
    "plt.legend(labels=[\"Explained Variance\",\"Cumulative Explained Variance\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming into elbow curve\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=no_of_components, y=exp_variance_ratio, color='green')\n",
    "plt.xlim(2.5, 18.5)\n",
    "plt.ylim(0,0.05)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating slope of cumulative explained variance:\n",
    "\n",
    "differences = [i-exp_variance_ratio[1+exp_variance_ratio.index(i)] for i in exp_variance_ratio[3:49]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdac206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better visualising 'elbows'. Need to pick poison.\n",
    "\n",
    "x = list(range(4,50))\n",
    "y = differences\n",
    "plt.figure(figsize=(18,5))\n",
    "sns.barplot(x=x, y=y, color='green')\n",
    "plt.title('Difference in Explained Variance between a Given Component and the Next One')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fdbd3",
   "metadata": {},
   "source": [
    "**Comment:** The larger the value in the above diagram, the more *useless* the next component. We can choose 9, 11, or 15 components. We can \"choose our poison\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba34ef",
   "metadata": {},
   "source": [
    "# Processing the Lyrics Features of All Songs and Playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e1100",
   "metadata": {},
   "source": [
    "Link to try: https://towardsdatascience.com/using-sentence-embeddings-to-explore-the-similarities-and-differences-in-song-lyrics-1820ac713f00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86742f",
   "metadata": {},
   "source": [
    "**Comment:** File formatted properly. Text file for 'fails' will need to be redone.\n",
    "\n",
    "**Update:** Redone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4616da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Song Files\n",
    "\n",
    "# Loading csv\n",
    "originals_lyrics = pd.read_csv('song_lyrics_func1.csv')\n",
    "remixes_lyrics = pd.read_csv('song_lyrics_func2.csv')\n",
    "lyrics_fixed = pd.read_csv('lyrics_sharaf_fix2.txt')\n",
    "\n",
    "# Making necessary changes\n",
    "originals_lyrics.drop('Unnamed: 0', axis=1, inplace=True) # Forgot to remove index before saving as csv\n",
    "remixes_lyrics.drop('Unnamed: 0', axis=1, inplace=True) # Forgot to remove index before saving as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820f782d",
   "metadata": {},
   "source": [
    "**Note:** We will need to append originals and remixes on top of lyrics_fixed, as removing duplicates will keep the first occurance of a row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the final lyrics\n",
    "\n",
    "# Concatenate\n",
    "final_lyrics = pd.concat([lyrics_fixed.copy(), \n",
    "                          originals_lyrics.copy(), \n",
    "                          remixes_lyrics.copy()]).drop_duplicates(subset=['track_name', 'artist_name'])\n",
    "\n",
    "# Drop the ones with fail\n",
    "final_lyrics = final_lyrics[final_lyrics.lyrics != 'Fail']\n",
    "\n",
    "# Making necessary changes to manipulate data\n",
    "final_lyrics.lyrics = final_lyrics.lyrics.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to clean lyrics\n",
    "\n",
    "# Specifying what to remove\n",
    "remove_from_lyrics = ['\\n', 'Lyrics', '[Verse 1]', '[Verse 2]', '[Verse 2]', '[Intro]', \n",
    "                      '[Chorus]', '[Post-Chorus]', '[Bridge]', '[Outro]']\n",
    "\n",
    "# Create function \n",
    "def clean_lyrics(lyrics_df):\n",
    "    for element in remove_from_lyrics:\n",
    "        lyrics_df.lyrics = lyrics_df.lyrics.apply(lambda x: x.replace(element, \" \") if element in x else x)\n",
    "    return lyrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to clean df\n",
    "\n",
    "final_lyrics_cleaned = clean_lyrics(final_lyrics)\n",
    "final_lyrics_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d457a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an index for each pair for reference\n",
    "\n",
    "final_lyrics_cleaned['primary_key'] = range(1, final_lyrics_cleaned.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0549c2f1",
   "metadata": {},
   "source": [
    "### Attempting to do Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model to do embeddings:\n",
    "\n",
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'\n",
    "embedder = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f37a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model to embed\n",
    "\n",
    "def embed_text(input):\n",
    "    return np.array(embedder(input)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9209f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of lists\n",
    "\n",
    "def embedded_lyrics_df(df):\n",
    "    embedded_lyrics = []\n",
    "    for lyrics in df.lyrics.tolist():\n",
    "        temp = embed_text(lyrics)\n",
    "        embedded_lyrics.append(temp)\n",
    "    temp_df = pd.DataFrame(embedded_lyrics)\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d32f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function\n",
    "\n",
    "# embedded_lyrics_df1 = embedded_lyrics_df(final_lyrics_cleaned)\n",
    "\n",
    "# save\n",
    "# embedded_lyrics_df1.to_csv('3639_lyrics_embeddings.csv')\n",
    "\n",
    "# load data\n",
    "embedded_lyrics_df1 = pd.read_csv('3639_lyrics_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905edd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Shape\n",
    "\n",
    "embedded_lyrics_df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with embeddings\n",
    "\n",
    "# Getting df ready\n",
    "merge_with_embeddings_df = df[['playlist_id', 'playlist_name', 'track_id', 'track_name', 'artist_name']].copy()\n",
    "\n",
    "# Concatenating with columns to join upon\n",
    "embedded_lyrics_df2 = pd.concat([final_lyrics_cleaned[['track_name', 'artist_name']].copy().reset_index(drop=True), \n",
    "                                 embedded_lyrics_df1.copy().reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Merging\n",
    "merged_embeddings = merge_with_embeddings_df.merge(embedded_lyrics_df2, how='left', on=['track_name', 'artist_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by each vector by their playlists\n",
    "\n",
    "pivots_embeddings = []\n",
    "\n",
    "for column in merged_embeddings.columns[6:]:\n",
    "    embeddings_playlist_pivot = pd.pivot_table(merged_embeddings, \n",
    "                                           index='artist_name', \n",
    "                                           columns='playlist_id', \n",
    "                                           aggfunc = np.mean,\n",
    "                                           values=column).reset_index()\n",
    "    pivots_embeddings.append(embeddings_playlist_pivot)\n",
    "\n",
    "pca_ready_embeddings = pd.concat(pivots_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257dfe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing format for better storage\n",
    "\n",
    "pca_ready_embeddings = pca_ready_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6ed77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_ready_embeddings.to_csv('pca_ready_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store artist name record in given order\n",
    "\n",
    "# Store names\n",
    "store_artists_pca_embeddings = pca_ready_embeddings['artist_name'].copy()\n",
    "\n",
    "# Drop the name to get ready for PCA:\n",
    "pca_ready_embeddings.drop('artist_name', axis=1, inplace=True)\n",
    "\n",
    "# fill nulls\n",
    "pca_ready_embeddings.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29406504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pca\n",
    "\n",
    "lyrics_pca = Pipeline([('scaler', StandardScaler()), ('pca', PCA(n_components=50))])\n",
    "lyrics_pca.fit(pca_ready_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603d83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running analysis\n",
    "\n",
    "no_of_components = list(range(1,51))\n",
    "exp_variance_ratio = lyrics_pca[1].explained_variance_ratio_.tolist()\n",
    "cum_exp_variance = np.cumsum(exp_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05203ca0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting explained variance and cumulative variance over components\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.lineplot(x=no_of_components, y=exp_variance_ratio, color='green')\n",
    "sns.lineplot(x=no_of_components, y=cum_exp_variance, color = 'orange')\n",
    "plt.legend(labels=[\"Explained Variance\",\"Cumulative Explained Variance\"])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming into elbow curve\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.barplot(x=no_of_components, y=exp_variance_ratio, color='green')\n",
    "plt.xlim(2.5, 18.5)\n",
    "plt.ylim(0,0.05)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5a8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better visualising 'elbows'. Need to pick poison.\n",
    "\n",
    "differences = [i-exp_variance_ratio[1+exp_variance_ratio.index(i)] for i in exp_variance_ratio[0:49]]\n",
    "\n",
    "x = list(range(2,50))\n",
    "y = differences[1:]\n",
    "plt.figure(figsize=(18,5))\n",
    "sns.barplot(x=x, y=y, color='green')\n",
    "plt.title('Difference in Explained Variance between a Given Component and the Next One')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b5ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
